{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive/\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z7NgYMl8-cq","outputId":"1bc19bfc-a80a-492f-8022-9677fe340451","execution":{"iopub.status.busy":"2023-01-18T04:40:41.609846Z","iopub.execute_input":"2023-01-18T04:40:41.610104Z","iopub.status.idle":"2023-01-18T04:40:41.613943Z","shell.execute_reply.started":"2023-01-18T04:40:41.610080Z","shell.execute_reply":"2023-01-18T04:40:41.613251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:40:41.618344Z","iopub.execute_input":"2023-01-18T04:40:41.618603Z","iopub.status.idle":"2023-01-18T04:40:46.389526Z","shell.execute_reply.started":"2023-01-18T04:40:41.618579Z","shell.execute_reply":"2023-01-18T04:40:46.388504Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.8/site-packages (from gdown) (2.28.1)\nCollecting filelock\n  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.13)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\nCollecting PySocks!=1.5.7,>=1.5.6\n  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\nInstalling collected packages: PySocks, filelock, gdown\nSuccessfully installed PySocks-1.7.1 filelock-3.9.0 gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1-Y70PaxrtT9suVN3a9jZkvU932VPUR4K","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:40:46.391437Z","iopub.execute_input":"2023-01-18T04:40:46.391756Z","iopub.status.idle":"2023-01-18T04:40:48.121024Z","shell.execute_reply.started":"2023-01-18T04:40:46.391719Z","shell.execute_reply":"2023-01-18T04:40:48.120063Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-Y70PaxrtT9suVN3a9jZkvU932VPUR4K\nTo: /kaggle/working/sequences.txt\n100%|██████████████████████████████████████| 1.58M/1.58M [00:00<00:00, 97.4MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1-Pm4dOGP8n8eXx458mNSOKC4dLS5aQc8","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:40:48.122407Z","iopub.execute_input":"2023-01-18T04:40:48.122724Z","iopub.status.idle":"2023-01-18T04:40:49.689203Z","shell.execute_reply.started":"2023-01-18T04:40:48.122695Z","shell.execute_reply":"2023-01-18T04:40:49.687958Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-Pm4dOGP8n8eXx458mNSOKC4dLS5aQc8\nTo: /kaggle/working/graph_labels.txt\n100%|██████████████████████████████████████| 42.9k/42.9k [00:00<00:00, 47.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1fhHU72USC-wCxhMzIxh8qzy8IObeMCnQ","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:40:49.692168Z","iopub.execute_input":"2023-01-18T04:40:49.693131Z","iopub.status.idle":"2023-01-18T04:41:05.861376Z","shell.execute_reply.started":"2023-01-18T04:40:49.693083Z","shell.execute_reply":"2023-01-18T04:41:05.860042Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1fhHU72USC-wCxhMzIxh8qzy8IObeMCnQ\nTo: /kaggle/working/model_ProBert_1.pth\n100%|███████████████████████████████████████| 1.68G/1.68G [00:14<00:00, 116MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nos.chdir(\"/kaggle/working/\")","metadata":{"id":"K2whhzTY9Kfe","execution":{"iopub.status.busy":"2023-01-18T04:41:05.862879Z","iopub.execute_input":"2023-01-18T04:41:05.863196Z","iopub.status.idle":"2023-01-18T04:41:05.867635Z","shell.execute_reply.started":"2023-01-18T04:41:05.863168Z","shell.execute_reply":"2023-01-18T04:41:05.866748Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!git clone https://github.com/mani-aiml/amazon-sagemaker-protein-classification.git","metadata":{"id":"4aCfjcy49Vd7","execution":{"iopub.status.busy":"2023-01-18T04:41:05.868767Z","iopub.execute_input":"2023-01-18T04:41:05.869067Z","iopub.status.idle":"2023-01-18T04:41:05.878163Z","shell.execute_reply.started":"2023-01-18T04:41:05.869042Z","shell.execute_reply":"2023-01-18T04:41:05.877235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:41:05.879249Z","iopub.execute_input":"2023-01-18T04:41:05.879543Z","iopub.status.idle":"2023-01-18T04:41:22.421768Z","shell.execute_reply.started":"2023-01-18T04:41:05.879517Z","shell.execute_reply":"2023-01-18T04:41:22.421006Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n\nclass ProteinSequenceDataset(Dataset):\n    def __init__(self, sequence, targets, tokenizer, max_len):\n        self.sequence = sequence\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sequence)\n\n    def __getitem__(self, item):\n        sequence = str(self.sequence[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n            sequence,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n          'protein_sequence': sequence,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"id":"l6r0WSK2_qHv","execution":{"iopub.status.busy":"2023-01-18T04:41:22.422899Z","iopub.execute_input":"2023-01-18T04:41:22.423253Z","iopub.status.idle":"2023-01-18T04:41:22.433516Z","shell.execute_reply.started":"2023-01-18T04:41:22.423220Z","shell.execute_reply":"2023-01-18T04:41:22.432758Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJf5ADv_AXLa","outputId":"002d4176-3feb-4b74-c5c8-5ec3db57a942","execution":{"iopub.status.busy":"2023-01-18T04:41:22.434573Z","iopub.execute_input":"2023-01-18T04:41:22.434874Z","iopub.status.idle":"2023-01-18T04:41:45.556348Z","shell.execute_reply.started":"2023-01-18T04:41:22.434848Z","shell.execute_reply":"2023-01-18T04:41:45.555387Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nCollecting huggingface-hub<1.0,>=0.10.0\n  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.64.1)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.24.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (22.0)\nCollecting regex!=2019.12.17\n  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 KB\u001b[0m \u001b[31m341.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\nInstalling collected packages: tokenizers, regex, huggingface-hub, transformers\nSuccessfully installed huggingface-hub-0.11.1 regex-2022.10.31 tokenizers-0.13.2 transformers-4.25.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\nPRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\nclass ProteinClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(ProteinClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                        nn.Linear(self.bert.config.hidden_size, n_classes),\n                                        nn.Tanh())\n        \n    def forward(self, input_ids, attention_mask):\n        output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        return self.classifier(output.pooler_output)","metadata":{"id":"8uQQbS6-9fyt","execution":{"iopub.status.busy":"2023-01-18T04:41:45.559169Z","iopub.execute_input":"2023-01-18T04:41:45.559479Z","iopub.status.idle":"2023-01-18T04:42:04.966213Z","shell.execute_reply.started":"2023-01-18T04:41:45.559449Z","shell.execute_reply":"2023-01-18T04:42:04.965137Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2023-01-18 04:41:45.759941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-18 04:41:56.047284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-01-18 04:41:56.047421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-01-18 04:41:56.047434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[percpu.cc : 552] RAW: rseq syscall failed with errno 1\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch_optimizer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"he-cvJOzAlhI","outputId":"a684910d-26c8-415b-8431-b7ac3937a57a","execution":{"iopub.status.busy":"2023-01-18T04:42:04.967508Z","iopub.execute_input":"2023-01-18T04:42:04.968017Z","iopub.status.idle":"2023-01-18T04:42:09.770212Z","shell.execute_reply.started":"2023-01-18T04:42:04.967987Z","shell.execute_reply":"2023-01-18T04:42:09.769227Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting torch_optimizer\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytorch-ranger>=0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.8/site-packages (from torch_optimizer) (1.13.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.10.3.66)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (4.4.0)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (8.5.0.96)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (57.5.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (0.38.4)\nInstalling collected packages: pytorch-ranger, torch_optimizer\nSuccessfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-01-18T04:42:09.771581Z","iopub.execute_input":"2023-01-18T04:42:09.771920Z","iopub.status.idle":"2023-01-18T04:42:17.427982Z","shell.execute_reply.started":"2023-01-18T04:42:09.771890Z","shell.execute_reply":"2023-01-18T04:42:17.426844Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting scikit-learn\n  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.10.0)\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.24.1)\nCollecting joblib>=1.1.1\n  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: threadpoolctl, joblib, scikit-learn\nSuccessfully installed joblib-1.2.0 scikit-learn-1.2.0 threadpoolctl-3.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%tb\n# from __future__ import print_function\n\nimport argparse\n# import json\nimport logging\nimport os\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\nfrom transformers import BertTokenizer, get_linear_schedule_with_warmup\nimport torch_optimizer as optim\n\n# Network definition\n# from model_def import ProteinClassifier\n# from data_prep import ProteinSequenceDataset\n \n## SageMaker Distributed code.\n# from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n# import smdistributed.dataparallel.torch.distributed as dist\n\n# dist.init_process_group()\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\nMAX_LEN = 512  # this is the max length of the sequence\nPRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)\n\ntraining_dir = \"/kaggle/working/\"\n\nsequences = list()\nprint(training_dir+'sequences.txt')\nwith open(training_dir+'sequences.txt', 'r') as f:\n    for line in f:\n        sequences.append(line[:-1])\n\n# Split data into training and test sets\nsequences_train = list()\nsequences_test = list()\nproteins_test = list()\ny_train = list()\nwith open(training_dir+'graph_labels.txt', 'r') as f:\n    for i,line in enumerate(f):\n        t = line.split(',')\n        if len(t[1][:-1]) == 0:\n            proteins_test.append(t[0])\n            sequences_test.append(sequences[i])\n        else:\n            sequences_train.append(sequences[i])\n            y_train.append(int(t[1][:-1]))\nfrom sklearn.model_selection import train_test_split\n\nsequences_train_ , sequences_val, y_train_, y_val = train_test_split(sequences_train,y_train ,\n                                   random_state=104, \n                                   test_size=0.1, \n                                   shuffle=True)\n  \ndef _get_train_data_loader(batch_size, training_dir):\n\n    # Read sequences\n#     sequences = list()\n#     print(training_dir+'sequences.txt')\n#     with open(training_dir+'sequences.txt', 'r') as f:\n#         for line in f:\n#             sequences.append(line[:-1])\n\n#     # Split data into training and test sets\n#     sequences_train = list()\n#     sequences_test = list()\n#     proteins_test = list()\n#     y_train = list()\n#     with open(training_dir+'graph_labels.txt', 'r') as f:\n#         for i,line in enumerate(f):\n#             t = line.split(',')\n#             if len(t[1][:-1]) == 0:\n#                 proteins_test.append(t[0])\n#                 sequences_test.append(sequences[i])\n#             else:\n#                 sequences_train.append(sequences[i])\n#                 y_train.append(int(t[1][:-1]))\n    sequences_train = sequences_train_\n    y_train =y_train_\n    sequences_train = np.array(sequences_train)\n    y_train = np.array(y_train)\n\n    train_data = ProteinSequenceDataset(\n        sequence=sequences_train,\n        targets=y_train,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n  )\n    # train_sampler = torch.utils.data.distributed.DistributedSampler(\n    #         dataset,\n    #         num_replicas=dist.get_world_size(),\n    #         rank=dist.get_rank())\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n    return train_dataloader\n\ndef _get_test_data_loader(batch_size, training_dir):\n        # Read sequences\n#     sequences = list()\n#     with open(training_dir+'sequences.txt', 'r') as f:\n#         for line in f:\n#             sequences.append(line[:-1])\n\n    # Split data into training and test sets\n#     sequences_train = list()\n#     sequences_test = list()\n#     proteins_test = list()\n#     y_train = list()\n#     with open(training_dir+'graph_labels.txt', 'r') as f:\n#         for i,line in enumerate(f):\n#             t = line.split(',')\n#             if len(t[1][:-1]) == 0:\n#                 proteins_test.append(t[0])\n#                 sequences_test.append(sequences[i])\n#             else:\n#                 sequences_train.append(sequences[i])\n#                 y_train.append(int(t[1][:-1]))\n    sequences_test = sequences_val\n    proteins_test = y_val\n    sequences_test = np.array(sequences_test)\n    proteins_test = np.array(proteins_test)\n\n    test_data = ProteinSequenceDataset(\n        sequence=sequences_test,\n        targets=proteins_test,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n  )\n    # test_sampler = RandomSampler(test_data)\n    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n    return test_dataloader\n\ndef freeze(model, frozen_layers):\n    modules = [model.bert.encoder.layer[:frozen_layers]] \n    for module in modules:\n        for param in module.parameters():\n            param.requires_grad = False\n            \ndef train(batch_size,data_dir,test_batch_size,test_dir,frozen_layers,num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir):\n    #use_cuda = args.num_gpus > 0\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # world_size = dist.get_world_size()\n    # rank = dist.get_rank()\n    # local_rank = dist.get_local_rank()\n    \n    # set the seed for generating random numbers\n    # torch.manual_seed(args.seed)\n    # if use_cuda:\n    #     torch.cuda.manual_seed(args.seed)\n\n    train_loader = _get_train_data_loader(batch_size, data_dir)\n    #if rank == 0:\n    test_loader = _get_test_data_loader(test_batch_size, test_dir)\n    print(\"Max length of sequence: \", MAX_LEN)\n    print(\"Freezing {} layers\".format(frozen_layers))\n    print(\"Model used: \", PRE_TRAINED_MODEL_NAME)\n\n    # logger.debug(\n    #     \"Processes {}/{} ({:.0f}%) of train data\".format(\n    #         len(train_loader.sampler),\n    #         len(train_loader.dataset),\n    #         100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n    #     ))\n\n    model = ProteinClassifier(\n        num_labels  # The number of output labels.\n    )\n    freeze(model, frozen_layers)\n    model = model.to(device) #DDP(model.to(device), broadcast_buffers=False)\n    # torch.cuda.set_device('local_rank')\n    # model.cuda(local_rank)\n    continue_training = True\n    \n    if continue_training :\n        print(\"=====PRETRAINED MODEL======\")\n        #load checkpoint\n        checkpoint = torch.load(\"/kaggle/working/model_ProBert_1.pth\")\n        #load state dict\n        model.load_state_dict(checkpoint)\n    \n    else:\n        print(\"=====Fine Tuning FROM ProBert======\")\n    optimizer = optim.Lamb(\n            model.parameters(), \n            lr = lr ,  #* dist.get_world_size()\n            betas=(0.9, 0.999), \n            eps=epsilon, \n            weight_decay=weight_decay)\n    total_steps = len(train_loader.dataset)\n    \n    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n    \n    loss_fn = nn.CrossEntropyLoss().to(device)\n        \n    for epoch in range(1, epochs + 1):\n        model.train()\n        for step, batch in enumerate(train_loader):\n            b_input_ids = batch['input_ids'].to(device)\n            b_input_mask = batch['attention_mask'].to(device)\n            b_labels = batch['targets'].to(device)\n\n            outputs = model(b_input_ids,attention_mask=b_input_mask)\n            loss = loss_fn(outputs, b_labels)\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            if step % log_interval == 0 : #and rank == 0\n                logger.info(\n                    \"Collecting data from Master Node: \\n Train Epoch: {} [{}/{} ({:.0f}%)] Training Loss: {:.6f}\".format(\n                        epoch,\n                        step * len(batch['input_ids']),  #*world_size\n                        len(train_loader.dataset),\n                        100.0 * step / len(train_loader),\n                        loss.item(),\n                    ))\n            if verbose:\n                print('Batch', step)\n        test(model, test_loader, device)\n        scheduler.step()\n        model_save = model.module if hasattr(model, \"module\") else model\n        save_model(model_save, model_dir, epoch)\n\ndef save_model(model, model_dir, i):\n    path = os.path.join(model_dir, f'model_ProBert_{i+1}.pth')\n    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n    torch.save(model.state_dict(), path)\n    logger.info(f\"Saving model: {path} \\n\")\n\n\ndef test(model, test_loader, device):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    tmp_eval_accuracy, eval_accuracy = 0, 0\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            b_input_ids = batch['input_ids'].to(device)\n            b_input_mask = batch['attention_mask'].to(device)\n            b_labels = batch['targets'].to(device)\n\n            outputs = model(b_input_ids,attention_mask=b_input_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, b_labels)\n            correct_predictions += torch.sum(preds == b_labels)\n            losses.append(loss.item())\n            \n    print('\\nTest set: Validation loss: {:.4f}, Validation Accuracy: {:.0f}%\\n'.format(\n        np.mean(losses),\n        100. * correct_predictions.double() / len(test_loader.dataset)))\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n\n#     # Data and model checkpoints directories\n#     parser.add_argument(\"--num_labels\", type=int, default=18, metavar=\"N\", help=\"input batch size for training (default: 10)\")\n\n#     parser.add_argument(\"--batch-size\", type=int, default=8, metavar=\"N\", help=\"input batch size for training (default: 1)\")\n#     parser.add_argument(\"--test-batch-size\", type=int, default=8, metavar=\"N\", help=\"input batch size for testing (default: 8)\")\n#     parser.add_argument(\"--epochs\", type=int, default=10, metavar=\"N\", help=\"number of epochs to train (default: 2)\")\n#     parser.add_argument(\"--lr\", type=float, default=0.3e-5, metavar=\"LR\", help=\"learning rate (default: 0.3e-5)\")\n#     parser.add_argument(\"--weight_decay\", type=float, default=0.01, metavar=\"M\", help=\"weight_decay (default: 0.01)\")\n#     parser.add_argument(\"--seed\", type=int, default=43, metavar=\"S\", help=\"random seed (default: 43)\")\n#     parser.add_argument(\"--epsilon\", type=int, default=1e-8, metavar=\"EP\", help=\"random seed (default: 1e-8)\")\n#     parser.add_argument(\"--frozen_layers\", type=int, default=10, metavar=\"NL\", help=\"number of frozen layers(default: 10)\")\n#     parser.add_argument('--verbose', action='store_true', default=False,help='For displaying SMDataParallel-specific logs')\n#     parser.add_argument(\"--log-interval\",type=int,default=10,metavar=\"N\",help=\"how many batches to wait before logging training status\",)\n   \n#     # Container environment\n#     # parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n#     # parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n#     parser.add_argument(\"--model-dir\", type=str, default=\"/Models\")\n#     parser.add_argument(\"--data-dir\", type=str, default=\"/content/drive/MyDrive/Challenge\")\n#     parser.add_argument(\"--test\", type=str, default=\"/content/drive/MyDrive/Challenge\")\n#     # parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n\n#     train(parser.parse_args())\n\nbatch_size = 8\ndata_dir = \"/kaggle/working/\"\ntest_batch_size = 8\ntest_dir = \"/kaggle/working/\"\nfrozen_layers = 15\nnum_labels = 18\nlr = 0.3e-5\nepsilon = 1e-8\nweight_decay = 0.01\nepochs = 10\nlog_interval = 10\nverbose = False\nmodel_dir =\"/kaggle/working/\"\ntrain(batch_size,data_dir,test_batch_size,test_dir,frozen_layers,num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"l89NLX8r9iXE","outputId":"93fa7231-644e-423a-89fb-aa4ed51ceb5a","execution":{"iopub.status.busy":"2023-01-18T05:08:19.300340Z","iopub.execute_input":"2023-01-18T05:08:19.300722Z"},"trusted":true},"execution_count":null,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 309\u001b[0m\n\u001b[1;32m    307\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    308\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 309\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfrozen_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[17], line 191\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size, data_dir, test_batch_size, test_dir, frozen_layers, num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir)\u001b[0m\n\u001b[1;32m    189\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/model_ProBert_1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m#load state dict\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=====Fine Tuning FROM ProBert======\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"],"ename":"KeyError","evalue":"'model_state_dict'","output_type":"error"},{"name":"stdout","text":"/kaggle/working/sequences.txt\nMax length of sequence:  512\nFreezing 15 layers\nModel used:  Rostlab/prot_bert\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"=====PRETRAINED MODEL======\nCollecting data from Master Node: \n Train Epoch: 1 [0/4399 (0%)] Training Loss: 2.781484\nCollecting data from Master Node: \n Train Epoch: 1 [0/4399 (0%)] Training Loss: 2.781484\nCollecting data from Master Node: \n Train Epoch: 1 [0/4399 (0%)] Training Loss: 2.781484\nCollecting data from Master Node: \n Train Epoch: 1 [0/4399 (0%)] Training Loss: 2.781484\nCollecting data from Master Node: \n Train Epoch: 1 [80/4399 (2%)] Training Loss: 2.822648\nCollecting data from Master Node: \n Train Epoch: 1 [80/4399 (2%)] Training Loss: 2.822648\nCollecting data from Master Node: \n Train Epoch: 1 [80/4399 (2%)] Training Loss: 2.822648\nCollecting data from Master Node: \n Train Epoch: 1 [80/4399 (2%)] Training Loss: 2.822648\nCollecting data from Master Node: \n Train Epoch: 1 [160/4399 (4%)] Training Loss: 2.819513\nCollecting data from Master Node: \n Train Epoch: 1 [160/4399 (4%)] Training Loss: 2.819513\nCollecting data from Master Node: \n Train Epoch: 1 [160/4399 (4%)] Training Loss: 2.819513\nCollecting data from Master Node: \n Train Epoch: 1 [160/4399 (4%)] Training Loss: 2.819513\nCollecting data from Master Node: \n Train Epoch: 1 [240/4399 (5%)] Training Loss: 2.768016\nCollecting data from Master Node: \n Train Epoch: 1 [240/4399 (5%)] Training Loss: 2.768016\nCollecting data from Master Node: \n Train Epoch: 1 [240/4399 (5%)] Training Loss: 2.768016\nCollecting data from Master Node: \n Train Epoch: 1 [240/4399 (5%)] Training Loss: 2.768016\nCollecting data from Master Node: \n Train Epoch: 1 [320/4399 (7%)] Training Loss: 2.840553\nCollecting data from Master Node: \n Train Epoch: 1 [320/4399 (7%)] Training Loss: 2.840553\nCollecting data from Master Node: \n Train Epoch: 1 [320/4399 (7%)] Training Loss: 2.840553\nCollecting data from Master Node: \n Train Epoch: 1 [320/4399 (7%)] Training Loss: 2.840553\nCollecting data from Master Node: \n Train Epoch: 1 [400/4399 (9%)] Training Loss: 2.836749\nCollecting data from Master Node: \n Train Epoch: 1 [400/4399 (9%)] Training Loss: 2.836749\nCollecting data from Master Node: \n Train Epoch: 1 [400/4399 (9%)] Training Loss: 2.836749\nCollecting data from Master Node: \n Train Epoch: 1 [400/4399 (9%)] Training Loss: 2.836749\nCollecting data from Master Node: \n Train Epoch: 1 [480/4399 (11%)] Training Loss: 2.834883\nCollecting data from Master Node: \n Train Epoch: 1 [480/4399 (11%)] Training Loss: 2.834883\nCollecting data from Master Node: \n Train Epoch: 1 [480/4399 (11%)] Training Loss: 2.834883\nCollecting data from Master Node: \n Train Epoch: 1 [480/4399 (11%)] Training Loss: 2.834883\nCollecting data from Master Node: \n Train Epoch: 1 [560/4399 (13%)] Training Loss: 2.817901\nCollecting data from Master Node: \n Train Epoch: 1 [560/4399 (13%)] Training Loss: 2.817901\nCollecting data from Master Node: \n Train Epoch: 1 [560/4399 (13%)] Training Loss: 2.817901\nCollecting data from Master Node: \n Train Epoch: 1 [560/4399 (13%)] Training Loss: 2.817901\nCollecting data from Master Node: \n Train Epoch: 1 [640/4399 (15%)] Training Loss: 2.839833\nCollecting data from Master Node: \n Train Epoch: 1 [640/4399 (15%)] Training Loss: 2.839833\nCollecting data from Master Node: \n Train Epoch: 1 [640/4399 (15%)] Training Loss: 2.839833\nCollecting data from Master Node: \n Train Epoch: 1 [640/4399 (15%)] Training Loss: 2.839833\nCollecting data from Master Node: \n Train Epoch: 1 [720/4399 (16%)] Training Loss: 2.830739\nCollecting data from Master Node: \n Train Epoch: 1 [720/4399 (16%)] Training Loss: 2.830739\nCollecting data from Master Node: \n Train Epoch: 1 [720/4399 (16%)] Training Loss: 2.830739\nCollecting data from Master Node: \n Train Epoch: 1 [720/4399 (16%)] Training Loss: 2.830739\nCollecting data from Master Node: \n Train Epoch: 1 [800/4399 (18%)] Training Loss: 2.831687\nCollecting data from Master Node: \n Train Epoch: 1 [800/4399 (18%)] Training Loss: 2.831687\nCollecting data from Master Node: \n Train Epoch: 1 [800/4399 (18%)] Training Loss: 2.831687\nCollecting data from Master Node: \n Train Epoch: 1 [800/4399 (18%)] Training Loss: 2.831687\nCollecting data from Master Node: \n Train Epoch: 1 [880/4399 (20%)] Training Loss: 2.868416\nCollecting data from Master Node: \n Train Epoch: 1 [880/4399 (20%)] Training Loss: 2.868416\nCollecting data from Master Node: \n Train Epoch: 1 [880/4399 (20%)] Training Loss: 2.868416\nCollecting data from Master Node: \n Train Epoch: 1 [880/4399 (20%)] Training Loss: 2.868416\nCollecting data from Master Node: \n Train Epoch: 1 [960/4399 (22%)] Training Loss: 2.718635\nCollecting data from Master Node: \n Train Epoch: 1 [960/4399 (22%)] Training Loss: 2.718635\nCollecting data from Master Node: \n Train Epoch: 1 [960/4399 (22%)] Training Loss: 2.718635\nCollecting data from Master Node: \n Train Epoch: 1 [960/4399 (22%)] Training Loss: 2.718635\nCollecting data from Master Node: \n Train Epoch: 1 [1040/4399 (24%)] Training Loss: 2.844503\nCollecting data from Master Node: \n Train Epoch: 1 [1040/4399 (24%)] Training Loss: 2.844503\nCollecting data from Master Node: \n Train Epoch: 1 [1040/4399 (24%)] Training Loss: 2.844503\nCollecting data from Master Node: \n Train Epoch: 1 [1040/4399 (24%)] Training Loss: 2.844503\nCollecting data from Master Node: \n Train Epoch: 1 [1120/4399 (25%)] Training Loss: 2.759197\nCollecting data from Master Node: \n Train Epoch: 1 [1120/4399 (25%)] Training Loss: 2.759197\nCollecting data from Master Node: \n Train Epoch: 1 [1120/4399 (25%)] Training Loss: 2.759197\nCollecting data from Master Node: \n Train Epoch: 1 [1120/4399 (25%)] Training Loss: 2.759197\nCollecting data from Master Node: \n Train Epoch: 1 [1200/4399 (27%)] Training Loss: 2.829373\nCollecting data from Master Node: \n Train Epoch: 1 [1200/4399 (27%)] Training Loss: 2.829373\nCollecting data from Master Node: \n Train Epoch: 1 [1200/4399 (27%)] Training Loss: 2.829373\nCollecting data from Master Node: \n Train Epoch: 1 [1200/4399 (27%)] Training Loss: 2.829373\nCollecting data from Master Node: \n Train Epoch: 1 [1280/4399 (29%)] Training Loss: 2.905289\nCollecting data from Master Node: \n Train Epoch: 1 [1280/4399 (29%)] Training Loss: 2.905289\nCollecting data from Master Node: \n Train Epoch: 1 [1280/4399 (29%)] Training Loss: 2.905289\nCollecting data from Master Node: \n Train Epoch: 1 [1280/4399 (29%)] Training Loss: 2.905289\nCollecting data from Master Node: \n Train Epoch: 1 [1360/4399 (31%)] Training Loss: 2.864263\nCollecting data from Master Node: \n Train Epoch: 1 [1360/4399 (31%)] Training Loss: 2.864263\nCollecting data from Master Node: \n Train Epoch: 1 [1360/4399 (31%)] Training Loss: 2.864263\nCollecting data from Master Node: \n Train Epoch: 1 [1360/4399 (31%)] Training Loss: 2.864263\nCollecting data from Master Node: \n Train Epoch: 1 [1440/4399 (33%)] Training Loss: 2.734137\nCollecting data from Master Node: \n Train Epoch: 1 [1440/4399 (33%)] Training Loss: 2.734137\nCollecting data from Master Node: \n Train Epoch: 1 [1440/4399 (33%)] Training Loss: 2.734137\nCollecting data from Master Node: \n Train Epoch: 1 [1440/4399 (33%)] Training Loss: 2.734137\nCollecting data from Master Node: \n Train Epoch: 1 [1520/4399 (35%)] Training Loss: 2.874918\nCollecting data from Master Node: \n Train Epoch: 1 [1520/4399 (35%)] Training Loss: 2.874918\nCollecting data from Master Node: \n Train Epoch: 1 [1520/4399 (35%)] Training Loss: 2.874918\nCollecting data from Master Node: \n Train Epoch: 1 [1520/4399 (35%)] Training Loss: 2.874918\nCollecting data from Master Node: \n Train Epoch: 1 [1600/4399 (36%)] Training Loss: 2.772838\nCollecting data from Master Node: \n Train Epoch: 1 [1600/4399 (36%)] Training Loss: 2.772838\nCollecting data from Master Node: \n Train Epoch: 1 [1600/4399 (36%)] Training Loss: 2.772838\nCollecting data from Master Node: \n Train Epoch: 1 [1600/4399 (36%)] Training Loss: 2.772838\nCollecting data from Master Node: \n Train Epoch: 1 [1680/4399 (38%)] Training Loss: 2.797684\nCollecting data from Master Node: \n Train Epoch: 1 [1680/4399 (38%)] Training Loss: 2.797684\nCollecting data from Master Node: \n Train Epoch: 1 [1680/4399 (38%)] Training Loss: 2.797684\nCollecting data from Master Node: \n Train Epoch: 1 [1680/4399 (38%)] Training Loss: 2.797684\nCollecting data from Master Node: \n Train Epoch: 1 [1760/4399 (40%)] Training Loss: 2.788790\nCollecting data from Master Node: \n Train Epoch: 1 [1760/4399 (40%)] Training Loss: 2.788790\nCollecting data from Master Node: \n Train Epoch: 1 [1760/4399 (40%)] Training Loss: 2.788790\nCollecting data from Master Node: \n Train Epoch: 1 [1760/4399 (40%)] Training Loss: 2.788790\nCollecting data from Master Node: \n Train Epoch: 1 [1840/4399 (42%)] Training Loss: 2.801966\nCollecting data from Master Node: \n Train Epoch: 1 [1840/4399 (42%)] Training Loss: 2.801966\nCollecting data from Master Node: \n Train Epoch: 1 [1840/4399 (42%)] Training Loss: 2.801966\nCollecting data from Master Node: \n Train Epoch: 1 [1840/4399 (42%)] Training Loss: 2.801966\nCollecting data from Master Node: \n Train Epoch: 1 [1920/4399 (44%)] Training Loss: 2.838230\nCollecting data from Master Node: \n Train Epoch: 1 [1920/4399 (44%)] Training Loss: 2.838230\nCollecting data from Master Node: \n Train Epoch: 1 [1920/4399 (44%)] Training Loss: 2.838230\nCollecting data from Master Node: \n Train Epoch: 1 [1920/4399 (44%)] Training Loss: 2.838230\nCollecting data from Master Node: \n Train Epoch: 1 [2000/4399 (45%)] Training Loss: 2.764920\nCollecting data from Master Node: \n Train Epoch: 1 [2000/4399 (45%)] Training Loss: 2.764920\nCollecting data from Master Node: \n Train Epoch: 1 [2000/4399 (45%)] Training Loss: 2.764920\nCollecting data from Master Node: \n Train Epoch: 1 [2000/4399 (45%)] Training Loss: 2.764920\nCollecting data from Master Node: \n Train Epoch: 1 [2080/4399 (47%)] Training Loss: 2.818767\nCollecting data from Master Node: \n Train Epoch: 1 [2080/4399 (47%)] Training Loss: 2.818767\nCollecting data from Master Node: \n Train Epoch: 1 [2080/4399 (47%)] Training Loss: 2.818767\nCollecting data from Master Node: \n Train Epoch: 1 [2080/4399 (47%)] Training Loss: 2.818767\nCollecting data from Master Node: \n Train Epoch: 1 [2160/4399 (49%)] Training Loss: 2.874655\nCollecting data from Master Node: \n Train Epoch: 1 [2160/4399 (49%)] Training Loss: 2.874655\nCollecting data from Master Node: \n Train Epoch: 1 [2160/4399 (49%)] Training Loss: 2.874655\nCollecting data from Master Node: \n Train Epoch: 1 [2160/4399 (49%)] Training Loss: 2.874655\nCollecting data from Master Node: \n Train Epoch: 1 [2240/4399 (51%)] Training Loss: 2.886135\nCollecting data from Master Node: \n Train Epoch: 1 [2240/4399 (51%)] Training Loss: 2.886135\nCollecting data from Master Node: \n Train Epoch: 1 [2240/4399 (51%)] Training Loss: 2.886135\nCollecting data from Master Node: \n Train Epoch: 1 [2240/4399 (51%)] Training Loss: 2.886135\nCollecting data from Master Node: \n Train Epoch: 1 [2320/4399 (53%)] Training Loss: 2.843418\nCollecting data from Master Node: \n Train Epoch: 1 [2320/4399 (53%)] Training Loss: 2.843418\nCollecting data from Master Node: \n Train Epoch: 1 [2320/4399 (53%)] Training Loss: 2.843418\nCollecting data from Master Node: \n Train Epoch: 1 [2320/4399 (53%)] Training Loss: 2.843418\nCollecting data from Master Node: \n Train Epoch: 1 [2400/4399 (55%)] Training Loss: 2.761374\nCollecting data from Master Node: \n Train Epoch: 1 [2400/4399 (55%)] Training Loss: 2.761374\nCollecting data from Master Node: \n Train Epoch: 1 [2400/4399 (55%)] Training Loss: 2.761374\nCollecting data from Master Node: \n Train Epoch: 1 [2400/4399 (55%)] Training Loss: 2.761374\nCollecting data from Master Node: \n Train Epoch: 1 [2480/4399 (56%)] Training Loss: 2.834025\nCollecting data from Master Node: \n Train Epoch: 1 [2480/4399 (56%)] Training Loss: 2.834025\nCollecting data from Master Node: \n Train Epoch: 1 [2480/4399 (56%)] Training Loss: 2.834025\nCollecting data from Master Node: \n Train Epoch: 1 [2480/4399 (56%)] Training Loss: 2.834025\nCollecting data from Master Node: \n Train Epoch: 1 [2560/4399 (58%)] Training Loss: 2.745439\nCollecting data from Master Node: \n Train Epoch: 1 [2560/4399 (58%)] Training Loss: 2.745439\nCollecting data from Master Node: \n Train Epoch: 1 [2560/4399 (58%)] Training Loss: 2.745439\nCollecting data from Master Node: \n Train Epoch: 1 [2560/4399 (58%)] Training Loss: 2.745439\nCollecting data from Master Node: \n Train Epoch: 1 [2640/4399 (60%)] Training Loss: 2.783116\nCollecting data from Master Node: \n Train Epoch: 1 [2640/4399 (60%)] Training Loss: 2.783116\nCollecting data from Master Node: \n Train Epoch: 1 [2640/4399 (60%)] Training Loss: 2.783116\nCollecting data from Master Node: \n Train Epoch: 1 [2640/4399 (60%)] Training Loss: 2.783116\nCollecting data from Master Node: \n Train Epoch: 1 [2720/4399 (62%)] Training Loss: 2.742431\nCollecting data from Master Node: \n Train Epoch: 1 [2720/4399 (62%)] Training Loss: 2.742431\nCollecting data from Master Node: \n Train Epoch: 1 [2720/4399 (62%)] Training Loss: 2.742431\nCollecting data from Master Node: \n Train Epoch: 1 [2720/4399 (62%)] Training Loss: 2.742431\nCollecting data from Master Node: \n Train Epoch: 1 [2800/4399 (64%)] Training Loss: 2.784149\nCollecting data from Master Node: \n Train Epoch: 1 [2800/4399 (64%)] Training Loss: 2.784149\nCollecting data from Master Node: \n Train Epoch: 1 [2800/4399 (64%)] Training Loss: 2.784149\nCollecting data from Master Node: \n Train Epoch: 1 [2800/4399 (64%)] Training Loss: 2.784149\nCollecting data from Master Node: \n Train Epoch: 1 [2880/4399 (65%)] Training Loss: 2.744550\nCollecting data from Master Node: \n Train Epoch: 1 [2880/4399 (65%)] Training Loss: 2.744550\nCollecting data from Master Node: \n Train Epoch: 1 [2880/4399 (65%)] Training Loss: 2.744550\nCollecting data from Master Node: \n Train Epoch: 1 [2880/4399 (65%)] Training Loss: 2.744550\nCollecting data from Master Node: \n Train Epoch: 1 [2960/4399 (67%)] Training Loss: 2.935849\nCollecting data from Master Node: \n Train Epoch: 1 [2960/4399 (67%)] Training Loss: 2.935849\nCollecting data from Master Node: \n Train Epoch: 1 [2960/4399 (67%)] Training Loss: 2.935849\nCollecting data from Master Node: \n Train Epoch: 1 [2960/4399 (67%)] Training Loss: 2.935849\nCollecting data from Master Node: \n Train Epoch: 1 [3040/4399 (69%)] Training Loss: 2.856155\nCollecting data from Master Node: \n Train Epoch: 1 [3040/4399 (69%)] Training Loss: 2.856155\nCollecting data from Master Node: \n Train Epoch: 1 [3040/4399 (69%)] Training Loss: 2.856155\nCollecting data from Master Node: \n Train Epoch: 1 [3040/4399 (69%)] Training Loss: 2.856155\nCollecting data from Master Node: \n Train Epoch: 1 [3120/4399 (71%)] Training Loss: 2.826485\nCollecting data from Master Node: \n Train Epoch: 1 [3120/4399 (71%)] Training Loss: 2.826485\nCollecting data from Master Node: \n Train Epoch: 1 [3120/4399 (71%)] Training Loss: 2.826485\nCollecting data from Master Node: \n Train Epoch: 1 [3120/4399 (71%)] Training Loss: 2.826485\nCollecting data from Master Node: \n Train Epoch: 1 [3200/4399 (73%)] Training Loss: 2.815948\nCollecting data from Master Node: \n Train Epoch: 1 [3200/4399 (73%)] Training Loss: 2.815948\nCollecting data from Master Node: \n Train Epoch: 1 [3200/4399 (73%)] Training Loss: 2.815948\nCollecting data from Master Node: \n Train Epoch: 1 [3200/4399 (73%)] Training Loss: 2.815948\nCollecting data from Master Node: \n Train Epoch: 1 [3280/4399 (75%)] Training Loss: 2.892444\nCollecting data from Master Node: \n Train Epoch: 1 [3280/4399 (75%)] Training Loss: 2.892444\nCollecting data from Master Node: \n Train Epoch: 1 [3280/4399 (75%)] Training Loss: 2.892444\nCollecting data from Master Node: \n Train Epoch: 1 [3280/4399 (75%)] Training Loss: 2.892444\nCollecting data from Master Node: \n Train Epoch: 1 [3360/4399 (76%)] Training Loss: 2.806853\nCollecting data from Master Node: \n Train Epoch: 1 [3360/4399 (76%)] Training Loss: 2.806853\nCollecting data from Master Node: \n Train Epoch: 1 [3360/4399 (76%)] Training Loss: 2.806853\nCollecting data from Master Node: \n Train Epoch: 1 [3360/4399 (76%)] Training Loss: 2.806853\nCollecting data from Master Node: \n Train Epoch: 1 [3440/4399 (78%)] Training Loss: 2.821804\nCollecting data from Master Node: \n Train Epoch: 1 [3440/4399 (78%)] Training Loss: 2.821804\nCollecting data from Master Node: \n Train Epoch: 1 [3440/4399 (78%)] Training Loss: 2.821804\nCollecting data from Master Node: \n Train Epoch: 1 [3440/4399 (78%)] Training Loss: 2.821804\nCollecting data from Master Node: \n Train Epoch: 1 [3520/4399 (80%)] Training Loss: 2.855438\nCollecting data from Master Node: \n Train Epoch: 1 [3520/4399 (80%)] Training Loss: 2.855438\nCollecting data from Master Node: \n Train Epoch: 1 [3520/4399 (80%)] Training Loss: 2.855438\nCollecting data from Master Node: \n Train Epoch: 1 [3520/4399 (80%)] Training Loss: 2.855438\nCollecting data from Master Node: \n Train Epoch: 1 [3600/4399 (82%)] Training Loss: 2.831134\nCollecting data from Master Node: \n Train Epoch: 1 [3600/4399 (82%)] Training Loss: 2.831134\nCollecting data from Master Node: \n Train Epoch: 1 [3600/4399 (82%)] Training Loss: 2.831134\nCollecting data from Master Node: \n Train Epoch: 1 [3600/4399 (82%)] Training Loss: 2.831134\nCollecting data from Master Node: \n Train Epoch: 1 [3680/4399 (84%)] Training Loss: 2.793581\nCollecting data from Master Node: \n Train Epoch: 1 [3680/4399 (84%)] Training Loss: 2.793581\nCollecting data from Master Node: \n Train Epoch: 1 [3680/4399 (84%)] Training Loss: 2.793581\nCollecting data from Master Node: \n Train Epoch: 1 [3680/4399 (84%)] Training Loss: 2.793581\nCollecting data from Master Node: \n Train Epoch: 1 [3760/4399 (85%)] Training Loss: 2.846648\nCollecting data from Master Node: \n Train Epoch: 1 [3760/4399 (85%)] Training Loss: 2.846648\nCollecting data from Master Node: \n Train Epoch: 1 [3760/4399 (85%)] Training Loss: 2.846648\nCollecting data from Master Node: \n Train Epoch: 1 [3760/4399 (85%)] Training Loss: 2.846648\nCollecting data from Master Node: \n Train Epoch: 1 [3840/4399 (87%)] Training Loss: 2.920260\nCollecting data from Master Node: \n Train Epoch: 1 [3840/4399 (87%)] Training Loss: 2.920260\nCollecting data from Master Node: \n Train Epoch: 1 [3840/4399 (87%)] Training Loss: 2.920260\nCollecting data from Master Node: \n Train Epoch: 1 [3840/4399 (87%)] Training Loss: 2.920260\nCollecting data from Master Node: \n Train Epoch: 1 [3920/4399 (89%)] Training Loss: 2.861948\nCollecting data from Master Node: \n Train Epoch: 1 [3920/4399 (89%)] Training Loss: 2.861948\nCollecting data from Master Node: \n Train Epoch: 1 [3920/4399 (89%)] Training Loss: 2.861948\nCollecting data from Master Node: \n Train Epoch: 1 [3920/4399 (89%)] Training Loss: 2.861948\nCollecting data from Master Node: \n Train Epoch: 1 [4000/4399 (91%)] Training Loss: 2.815321\nCollecting data from Master Node: \n Train Epoch: 1 [4000/4399 (91%)] Training Loss: 2.815321\nCollecting data from Master Node: \n Train Epoch: 1 [4000/4399 (91%)] Training Loss: 2.815321\nCollecting data from Master Node: \n Train Epoch: 1 [4000/4399 (91%)] Training Loss: 2.815321\nCollecting data from Master Node: \n Train Epoch: 1 [4080/4399 (93%)] Training Loss: 2.767681\nCollecting data from Master Node: \n Train Epoch: 1 [4080/4399 (93%)] Training Loss: 2.767681\nCollecting data from Master Node: \n Train Epoch: 1 [4080/4399 (93%)] Training Loss: 2.767681\nCollecting data from Master Node: \n Train Epoch: 1 [4080/4399 (93%)] Training Loss: 2.767681\nCollecting data from Master Node: \n Train Epoch: 1 [4160/4399 (95%)] Training Loss: 2.764090\nCollecting data from Master Node: \n Train Epoch: 1 [4160/4399 (95%)] Training Loss: 2.764090\nCollecting data from Master Node: \n Train Epoch: 1 [4160/4399 (95%)] Training Loss: 2.764090\nCollecting data from Master Node: \n Train Epoch: 1 [4160/4399 (95%)] Training Loss: 2.764090\nCollecting data from Master Node: \n Train Epoch: 1 [4240/4399 (96%)] Training Loss: 2.819965\nCollecting data from Master Node: \n Train Epoch: 1 [4240/4399 (96%)] Training Loss: 2.819965\nCollecting data from Master Node: \n Train Epoch: 1 [4240/4399 (96%)] Training Loss: 2.819965\nCollecting data from Master Node: \n Train Epoch: 1 [4240/4399 (96%)] Training Loss: 2.819965\nCollecting data from Master Node: \n Train Epoch: 1 [4320/4399 (98%)] Training Loss: 2.791940\nCollecting data from Master Node: \n Train Epoch: 1 [4320/4399 (98%)] Training Loss: 2.791940\nCollecting data from Master Node: \n Train Epoch: 1 [4320/4399 (98%)] Training Loss: 2.791940\nCollecting data from Master Node: \n Train Epoch: 1 [4320/4399 (98%)] Training Loss: 2.791940\n\nTest set: Validation loss: 2.8065, Validation Accuracy: 20%\n\nSaving model: /kaggle/working/model_ProBert_2.pth \n\nSaving model: /kaggle/working/model_ProBert_2.pth \n\nSaving model: /kaggle/working/model_ProBert_2.pth \n\nSaving model: /kaggle/working/model_ProBert_2.pth \n\nCollecting data from Master Node: \n Train Epoch: 2 [0/4399 (0%)] Training Loss: 2.779567\nCollecting data from Master Node: \n Train Epoch: 2 [0/4399 (0%)] Training Loss: 2.779567\nCollecting data from Master Node: \n Train Epoch: 2 [0/4399 (0%)] Training Loss: 2.779567\nCollecting data from Master Node: \n Train Epoch: 2 [0/4399 (0%)] Training Loss: 2.779567\nCollecting data from Master Node: \n Train Epoch: 2 [80/4399 (2%)] Training Loss: 2.815844\nCollecting data from Master Node: \n Train Epoch: 2 [80/4399 (2%)] Training Loss: 2.815844\nCollecting data from Master Node: \n Train Epoch: 2 [80/4399 (2%)] Training Loss: 2.815844\nCollecting data from Master Node: \n Train Epoch: 2 [80/4399 (2%)] Training Loss: 2.815844\nCollecting data from Master Node: \n Train Epoch: 2 [160/4399 (4%)] Training Loss: 2.812797\nCollecting data from Master Node: \n Train Epoch: 2 [160/4399 (4%)] Training Loss: 2.812797\nCollecting data from Master Node: \n Train Epoch: 2 [160/4399 (4%)] Training Loss: 2.812797\nCollecting data from Master Node: \n Train Epoch: 2 [160/4399 (4%)] Training Loss: 2.812797\n","output_type":"stream"}]}]}