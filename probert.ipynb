{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive/\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z7NgYMl8-cq","outputId":"1bc19bfc-a80a-492f-8022-9677fe340451","execution":{"iopub.status.busy":"2023-01-19T00:05:40.547958Z","iopub.execute_input":"2023-01-19T00:05:40.548217Z","iopub.status.idle":"2023-01-19T00:05:40.552074Z","shell.execute_reply.started":"2023-01-19T00:05:40.548192Z","shell.execute_reply":"2023-01-19T00:05:40.551327Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:05:40.556760Z","iopub.execute_input":"2023-01-19T00:05:40.557447Z","iopub.status.idle":"2023-01-19T00:05:45.302877Z","shell.execute_reply.started":"2023-01-19T00:05:40.557420Z","shell.execute_reply":"2023-01-19T00:05:45.301724Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from gdown) (4.64.1)\nCollecting filelock\n  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.8/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.13)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests[socks]->gdown) (3.4)\nCollecting PySocks!=1.5.7,>=1.5.6\n  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\nInstalling collected packages: PySocks, filelock, gdown\nSuccessfully installed PySocks-1.7.1 filelock-3.9.0 gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1-Y70PaxrtT9suVN3a9jZkvU932VPUR4K","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:05:45.304793Z","iopub.execute_input":"2023-01-19T00:05:45.305087Z","iopub.status.idle":"2023-01-19T00:05:47.039081Z","shell.execute_reply.started":"2023-01-19T00:05:45.305057Z","shell.execute_reply":"2023-01-19T00:05:47.038144Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-Y70PaxrtT9suVN3a9jZkvU932VPUR4K\nTo: /kaggle/working/sequences.txt\n100%|███████████████████████████████████████| 1.58M/1.58M [00:00<00:00, 143MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1-Pm4dOGP8n8eXx458mNSOKC4dLS5aQc8","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:05:47.040457Z","iopub.execute_input":"2023-01-19T00:05:47.040751Z","iopub.status.idle":"2023-01-19T00:05:48.786288Z","shell.execute_reply.started":"2023-01-19T00:05:47.040722Z","shell.execute_reply":"2023-01-19T00:05:48.785135Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1-Pm4dOGP8n8eXx458mNSOKC4dLS5aQc8\nTo: /kaggle/working/graph_labels.txt\n100%|██████████████████████████████████████| 42.9k/42.9k [00:00<00:00, 50.9MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Prot_model\n!gdown --id 1MHWfO0Ym1wuagOckzUcL2aDMYiwr8f3c","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:05:48.788613Z","iopub.execute_input":"2023-01-19T00:05:48.788913Z","iopub.status.idle":"2023-01-19T00:06:04.087335Z","shell.execute_reply.started":"2023-01-19T00:05:48.788885Z","shell.execute_reply":"2023-01-19T00:06:04.086070Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.8/site-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1MHWfO0Ym1wuagOckzUcL2aDMYiwr8f3c\nTo: /kaggle/working/model_ProBert_3.pth\n100%|███████████████████████████████████████| 1.68G/1.68G [00:13<00:00, 122MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nos.chdir(\"/kaggle/working/\")","metadata":{"id":"K2whhzTY9Kfe","execution":{"iopub.status.busy":"2023-01-19T00:06:04.088856Z","iopub.execute_input":"2023-01-19T00:06:04.089175Z","iopub.status.idle":"2023-01-19T00:06:04.093842Z","shell.execute_reply.started":"2023-01-19T00:06:04.089144Z","shell.execute_reply":"2023-01-19T00:06:04.092935Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!git clone https://github.com/mani-aiml/amazon-sagemaker-protein-classification.git","metadata":{"id":"4aCfjcy49Vd7","execution":{"iopub.status.busy":"2023-01-19T00:06:04.095013Z","iopub.execute_input":"2023-01-19T00:06:04.095344Z","iopub.status.idle":"2023-01-19T00:06:04.106598Z","shell.execute_reply.started":"2023-01-19T00:06:04.095306Z","shell.execute_reply":"2023-01-19T00:06:04.105698Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:06:04.107666Z","iopub.execute_input":"2023-01-19T00:06:04.107937Z","iopub.status.idle":"2023-01-19T00:06:20.805598Z","shell.execute_reply.started":"2023-01-19T00:06:04.107913Z","shell.execute_reply":"2023-01-19T00:06:20.804644Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n\nclass ProteinSequenceDataset(Dataset):\n    def __init__(self, sequence, targets, tokenizer, max_len):\n        self.sequence = sequence\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sequence)\n\n    def __getitem__(self, item):\n        sequence = str(self.sequence[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n            sequence,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n          'protein_sequence': sequence,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","metadata":{"id":"l6r0WSK2_qHv","execution":{"iopub.status.busy":"2023-01-19T00:06:20.806891Z","iopub.execute_input":"2023-01-19T00:06:20.807219Z","iopub.status.idle":"2023-01-19T00:06:20.817169Z","shell.execute_reply.started":"2023-01-19T00:06:20.807194Z","shell.execute_reply":"2023-01-19T00:06:20.816330Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJf5ADv_AXLa","outputId":"002d4176-3feb-4b74-c5c8-5ec3db57a942","execution":{"iopub.status.busy":"2023-01-19T00:06:20.818282Z","iopub.execute_input":"2023-01-19T00:06:20.818673Z","iopub.status.idle":"2023-01-19T00:06:45.073659Z","shell.execute_reply.started":"2023-01-19T00:06:20.818647Z","shell.execute_reply":"2023-01-19T00:06:45.072601Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (22.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.64.1)\nCollecting huggingface-hub<1.0,>=0.10.0\n  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.24.1)\nCollecting regex!=2019.12.17\n  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 KB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\nInstalling collected packages: tokenizers, regex, huggingface-hub, transformers\nSuccessfully installed huggingface-hub-0.11.1 regex-2022.10.31 tokenizers-0.13.2 transformers-4.25.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\nPRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\nclass ProteinClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(ProteinClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                        nn.Linear(self.bert.config.hidden_size, n_classes),\n                                        nn.Tanh())\n        \n    def forward(self, input_ids, attention_mask):\n        output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        return self.classifier(output.pooler_output)","metadata":{"id":"8uQQbS6-9fyt","execution":{"iopub.status.busy":"2023-01-19T00:06:45.076745Z","iopub.execute_input":"2023-01-19T00:06:45.077133Z","iopub.status.idle":"2023-01-19T00:07:04.605072Z","shell.execute_reply.started":"2023-01-19T00:06:45.077098Z","shell.execute_reply":"2023-01-19T00:07:04.604199Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2023-01-19 00:06:45.285355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-19 00:06:55.815660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-01-19 00:06:55.815796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-01-19 00:06:55.815808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[percpu.cc : 552] RAW: rseq syscall failed with errno 1\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch_optimizer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"he-cvJOzAlhI","outputId":"a684910d-26c8-415b-8431-b7ac3937a57a","execution":{"iopub.status.busy":"2023-01-19T00:07:04.606290Z","iopub.execute_input":"2023-01-19T00:07:04.606820Z","iopub.status.idle":"2023-01-19T00:07:09.531361Z","shell.execute_reply.started":"2023-01-19T00:07:04.606790Z","shell.execute_reply":"2023-01-19T00:07:09.530202Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting torch_optimizer\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.8/site-packages (from torch_optimizer) (1.13.0)\nCollecting pytorch-ranger>=0.1.1\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.5.0->torch_optimizer) (4.4.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (57.5.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (0.38.4)\nInstalling collected packages: pytorch-ranger, torch_optimizer\nSuccessfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-01-19T00:07:09.532802Z","iopub.execute_input":"2023-01-19T00:07:09.533117Z","iopub.status.idle":"2023-01-19T00:07:17.085572Z","shell.execute_reply.started":"2023-01-19T00:07:09.533086Z","shell.execute_reply":"2023-01-19T00:07:17.084385Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting scikit-learn\n  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting joblib>=1.1.1\n  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.10.0)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from scikit-learn) (1.24.1)\nInstalling collected packages: threadpoolctl, joblib, scikit-learn\nSuccessfully installed joblib-1.2.0 scikit-learn-1.2.0 threadpoolctl-3.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%tb\n# from __future__ import print_function\n\nimport argparse\n# import json\nimport logging\nimport os\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\nfrom transformers import BertTokenizer, get_linear_schedule_with_warmup\nimport torch_optimizer as optim\n\n# Network definition\n# from model_def import ProteinClassifier\n# from data_prep import ProteinSequenceDataset\n \n## SageMaker Distributed code.\n# from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP\n# import smdistributed.dataparallel.torch.distributed as dist\n\n# dist.init_process_group()\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\nMAX_LEN = 512  # this is the max length of the sequence\nPRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)\n\ntraining_dir = \"/kaggle/working/\"\n\nsequences = list()\nprint(training_dir+'sequences.txt')\nwith open(training_dir+'sequences.txt', 'r') as f:\n    for line in f:\n        sequences.append(line[:-1])\n\n# Split data into training and test sets\nsequences_train = list()\nsequences_test = list()\nproteins_test = list()\ny_train = list()\nwith open(training_dir+'graph_labels.txt', 'r') as f:\n    for i,line in enumerate(f):\n        t = line.split(',')\n        if len(t[1][:-1]) == 0:\n            proteins_test.append(t[0])\n            sequences_test.append(sequences[i])\n        else:\n            sequences_train.append(sequences[i])\n            y_train.append(int(t[1][:-1]))\nfrom sklearn.model_selection import train_test_split\n\nsequences_train_ , sequences_val, y_train_, y_val = train_test_split(sequences_train,y_train ,\n                                   random_state=104, \n                                   test_size=0.1, \n                                   shuffle=True)\n  \ndef _get_train_data_loader(batch_size, training_dir):\n\n    # Read sequences\n#     sequences = list()\n#     print(training_dir+'sequences.txt')\n#     with open(training_dir+'sequences.txt', 'r') as f:\n#         for line in f:\n#             sequences.append(line[:-1])\n\n#     # Split data into training and test sets\n#     sequences_train = list()\n#     sequences_test = list()\n#     proteins_test = list()\n#     y_train = list()\n#     with open(training_dir+'graph_labels.txt', 'r') as f:\n#         for i,line in enumerate(f):\n#             t = line.split(',')\n#             if len(t[1][:-1]) == 0:\n#                 proteins_test.append(t[0])\n#                 sequences_test.append(sequences[i])\n#             else:\n#                 sequences_train.append(sequences[i])\n#                 y_train.append(int(t[1][:-1]))\n    sequences_train = sequences_train_\n    y_train =y_train_\n    sequences_train = np.array(sequences_train)\n    y_train = np.array(y_train)\n\n    train_data = ProteinSequenceDataset(\n        sequence=sequences_train,\n        targets=y_train,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n  )\n    # train_sampler = torch.utils.data.distributed.DistributedSampler(\n    #         dataset,\n    #         num_replicas=dist.get_world_size(),\n    #         rank=dist.get_rank())\n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n    return train_dataloader\n\ndef _get_test_data_loader(batch_size, training_dir):\n        # Read sequences\n#     sequences = list()\n#     with open(training_dir+'sequences.txt', 'r') as f:\n#         for line in f:\n#             sequences.append(line[:-1])\n\n    # Split data into training and test sets\n#     sequences_train = list()\n#     sequences_test = list()\n#     proteins_test = list()\n#     y_train = list()\n#     with open(training_dir+'graph_labels.txt', 'r') as f:\n#         for i,line in enumerate(f):\n#             t = line.split(',')\n#             if len(t[1][:-1]) == 0:\n#                 proteins_test.append(t[0])\n#                 sequences_test.append(sequences[i])\n#             else:\n#                 sequences_train.append(sequences[i])\n#                 y_train.append(int(t[1][:-1]))\n    sequences_test = sequences_val\n    proteins_test = y_val\n    sequences_test = np.array(sequences_test)\n    proteins_test = np.array(proteins_test)\n\n    test_data = ProteinSequenceDataset(\n        sequence=sequences_test,\n        targets=proteins_test,\n        tokenizer=tokenizer,\n        max_len=MAX_LEN\n  )\n    # test_sampler = RandomSampler(test_data)\n    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n    return test_dataloader\n\ndef freeze(model, frozen_layers):\n    modules = [model.bert.encoder.layer[:frozen_layers]] \n    for module in modules:\n        for param in module.parameters():\n            param.requires_grad = False\n            \ndef train(batch_size,data_dir,test_batch_size,test_dir,frozen_layers,num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir):\n    #use_cuda = args.num_gpus > 0\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # world_size = dist.get_world_size()\n    # rank = dist.get_rank()\n    # local_rank = dist.get_local_rank()\n    \n    # set the seed for generating random numbers\n    # torch.manual_seed(args.seed)\n    # if use_cuda:\n    #     torch.cuda.manual_seed(args.seed)\n\n    train_loader = _get_train_data_loader(batch_size, data_dir)\n    #if rank == 0:\n    test_loader = _get_test_data_loader(test_batch_size, test_dir)\n    print(\"Max length of sequence: \", MAX_LEN)\n    print(\"Freezing {} layers\".format(frozen_layers))\n    print(\"Model used: \", PRE_TRAINED_MODEL_NAME)\n\n    # logger.debug(\n    #     \"Processes {}/{} ({:.0f}%) of train data\".format(\n    #         len(train_loader.sampler),\n    #         len(train_loader.dataset),\n    #         100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n    #     ))\n\n    model = ProteinClassifier(\n        num_labels  # The number of output labels.\n    )\n    freeze(model, frozen_layers)\n    model = model.to(device) #DDP(model.to(device), broadcast_buffers=False)\n    # torch.cuda.set_device('local_rank')\n    # model.cuda(local_rank)\n    continue_training = True\n    \n    if continue_training :\n        print(\"=====PRETRAINED MODEL======\")\n        #load checkpoint\n        checkpoint = torch.load(\"/kaggle/working/model_ProBert_3.pth\")\n        #load state dict\n        model.load_state_dict(checkpoint)\n    \n    else:\n        print(\"=====Fine Tuning FROM ProBert======\")\n    optimizer = optim.Lamb(\n            model.parameters(), \n            lr = lr ,  #* dist.get_world_size()\n            betas=(0.9, 0.999), \n            eps=epsilon, \n            weight_decay=weight_decay)\n    total_steps = len(train_loader.dataset)\n    \n    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n    \n    loss_fn = nn.CrossEntropyLoss().to(device)\n        \n    for epoch in range(1, epochs + 1):\n        model.train()\n        for step, batch in enumerate(train_loader):\n            b_input_ids = batch['input_ids'].to(device)\n            b_input_mask = batch['attention_mask'].to(device)\n            b_labels = batch['targets'].to(device)\n\n            outputs = model(b_input_ids,attention_mask=b_input_mask)\n            loss = loss_fn(outputs, b_labels)\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # modified based on their gradients, the learning rate, etc.\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            if step % log_interval == 0 : #and rank == 0\n                logger.info(\n                    \"Collecting data from Master Node: \\n Train Epoch: {} [{}/{} ({:.0f}%)] Training Loss: {:.6f}\".format(\n                        epoch,\n                        step * len(batch['input_ids']),  #*world_size\n                        len(train_loader.dataset),\n                        100.0 * step / len(train_loader),\n                        loss.item(),\n                    ))\n            if verbose:\n                print('Batch', step)\n        test(model, test_loader, device)\n        scheduler.step()\n        model_save = model.module if hasattr(model, \"module\") else model\n        save_model(model_save, model_dir, epoch)\n\ndef save_model(model, model_dir, i):\n    path = os.path.join(model_dir, f'model_ProBert_{i+3}.pth')\n    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n    torch.save(model.state_dict(), path)\n    logger.info(f\"Saving model: {path} \\n\")\n\n\ndef test(model, test_loader, device):\n    model.eval()\n    losses = []\n    correct_predictions = 0\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    tmp_eval_accuracy, eval_accuracy = 0, 0\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            b_input_ids = batch['input_ids'].to(device)\n            b_input_mask = batch['attention_mask'].to(device)\n            b_labels = batch['targets'].to(device)\n\n            outputs = model(b_input_ids,attention_mask=b_input_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, b_labels)\n            correct_predictions += torch.sum(preds == b_labels)\n            losses.append(loss.item())\n            \n    print('\\nTest set: Validation loss: {:.4f}, Validation Accuracy: {:.0f}%\\n'.format(\n        np.mean(losses),\n        100. * correct_predictions.double() / len(test_loader.dataset)))\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n\n#     # Data and model checkpoints directories\n#     parser.add_argument(\"--num_labels\", type=int, default=18, metavar=\"N\", help=\"input batch size for training (default: 10)\")\n\n#     parser.add_argument(\"--batch-size\", type=int, default=8, metavar=\"N\", help=\"input batch size for training (default: 1)\")\n#     parser.add_argument(\"--test-batch-size\", type=int, default=8, metavar=\"N\", help=\"input batch size for testing (default: 8)\")\n#     parser.add_argument(\"--epochs\", type=int, default=10, metavar=\"N\", help=\"number of epochs to train (default: 2)\")\n#     parser.add_argument(\"--lr\", type=float, default=0.3e-5, metavar=\"LR\", help=\"learning rate (default: 0.3e-5)\")\n#     parser.add_argument(\"--weight_decay\", type=float, default=0.01, metavar=\"M\", help=\"weight_decay (default: 0.01)\")\n#     parser.add_argument(\"--seed\", type=int, default=43, metavar=\"S\", help=\"random seed (default: 43)\")\n#     parser.add_argument(\"--epsilon\", type=int, default=1e-8, metavar=\"EP\", help=\"random seed (default: 1e-8)\")\n#     parser.add_argument(\"--frozen_layers\", type=int, default=10, metavar=\"NL\", help=\"number of frozen layers(default: 10)\")\n#     parser.add_argument('--verbose', action='store_true', default=False,help='For displaying SMDataParallel-specific logs')\n#     parser.add_argument(\"--log-interval\",type=int,default=10,metavar=\"N\",help=\"how many batches to wait before logging training status\",)\n   \n#     # Container environment\n#     # parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n#     # parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n#     parser.add_argument(\"--model-dir\", type=str, default=\"/Models\")\n#     parser.add_argument(\"--data-dir\", type=str, default=\"/content/drive/MyDrive/Challenge\")\n#     parser.add_argument(\"--test\", type=str, default=\"/content/drive/MyDrive/Challenge\")\n#     # parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n\n#     train(parser.parse_args())\n\nbatch_size = 8\ndata_dir = \"/kaggle/working/\"\ntest_batch_size = 8\ntest_dir = \"/kaggle/working/\"\nfrozen_layers = 15\nnum_labels = 18\nlr = 0.3e-5\nepsilon = 1e-8\nweight_decay = 0.01\nepochs = 10\nlog_interval = 10\nverbose = False\nmodel_dir =\"/kaggle/working/\"\ntrain(batch_size,data_dir,test_batch_size,test_dir,frozen_layers,num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"l89NLX8r9iXE","outputId":"93fa7231-644e-423a-89fb-aa4ed51ceb5a","execution":{"iopub.status.busy":"2023-01-19T00:07:17.087886Z","iopub.execute_input":"2023-01-19T00:07:17.088209Z","iopub.status.idle":"2023-01-19T02:07:50.296342Z","shell.execute_reply.started":"2023-01-19T00:07:17.088179Z","shell.execute_reply":"2023-01-19T02:07:50.295032Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"No traceback available to show.\nDownloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 76.7kB/s]\nDownloading: 100%|██████████| 112/112 [00:00<00:00, 92.4kB/s]\nDownloading: 100%|██████████| 86.0/86.0 [00:00<00:00, 68.7kB/s]\nDownloading: 100%|██████████| 361/361 [00:00<00:00, 421kB/s]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working/sequences.txt\nMax length of sequence:  512\nFreezing 15 layers\nModel used:  Rostlab/prot_bert\n","output_type":"stream"},{"name":"stderr","text":"Downloading: 100%|██████████| 1.68G/1.68G [00:53<00:00, 31.5MB/s] \nSome weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"=====PRETRAINED MODEL======\nCollecting data from Master Node: \n Train Epoch: 1 [0/4399 (0%)] Training Loss: 2.785866\nCollecting data from Master Node: \n Train Epoch: 1 [80/4399 (2%)] Training Loss: 2.811027\nCollecting data from Master Node: \n Train Epoch: 1 [160/4399 (4%)] Training Loss: 2.784500\nCollecting data from Master Node: \n Train Epoch: 1 [240/4399 (5%)] Training Loss: 2.664945\nCollecting data from Master Node: \n Train Epoch: 1 [320/4399 (7%)] Training Loss: 2.850210\nCollecting data from Master Node: \n Train Epoch: 1 [400/4399 (9%)] Training Loss: 2.713369\nCollecting data from Master Node: \n Train Epoch: 1 [480/4399 (11%)] Training Loss: 2.791614\nCollecting data from Master Node: \n Train Epoch: 1 [560/4399 (13%)] Training Loss: 2.841779\nCollecting data from Master Node: \n Train Epoch: 1 [640/4399 (15%)] Training Loss: 2.779568\nCollecting data from Master Node: \n Train Epoch: 1 [720/4399 (16%)] Training Loss: 2.841230\nCollecting data from Master Node: \n Train Epoch: 1 [800/4399 (18%)] Training Loss: 2.799253\nCollecting data from Master Node: \n Train Epoch: 1 [880/4399 (20%)] Training Loss: 2.798998\nCollecting data from Master Node: \n Train Epoch: 1 [960/4399 (22%)] Training Loss: 2.672273\nCollecting data from Master Node: \n Train Epoch: 1 [1040/4399 (24%)] Training Loss: 2.772959\nCollecting data from Master Node: \n Train Epoch: 1 [1120/4399 (25%)] Training Loss: 2.695515\nCollecting data from Master Node: \n Train Epoch: 1 [1200/4399 (27%)] Training Loss: 2.826844\nCollecting data from Master Node: \n Train Epoch: 1 [1280/4399 (29%)] Training Loss: 2.777010\nCollecting data from Master Node: \n Train Epoch: 1 [1360/4399 (31%)] Training Loss: 2.796596\nCollecting data from Master Node: \n Train Epoch: 1 [1440/4399 (33%)] Training Loss: 2.692826\nCollecting data from Master Node: \n Train Epoch: 1 [1520/4399 (35%)] Training Loss: 2.905810\nCollecting data from Master Node: \n Train Epoch: 1 [1600/4399 (36%)] Training Loss: 2.775567\nCollecting data from Master Node: \n Train Epoch: 1 [1680/4399 (38%)] Training Loss: 2.783659\nCollecting data from Master Node: \n Train Epoch: 1 [1760/4399 (40%)] Training Loss: 2.814550\nCollecting data from Master Node: \n Train Epoch: 1 [1840/4399 (42%)] Training Loss: 2.720107\nCollecting data from Master Node: \n Train Epoch: 1 [1920/4399 (44%)] Training Loss: 2.760020\nCollecting data from Master Node: \n Train Epoch: 1 [2000/4399 (45%)] Training Loss: 2.753936\nCollecting data from Master Node: \n Train Epoch: 1 [2080/4399 (47%)] Training Loss: 2.781802\nCollecting data from Master Node: \n Train Epoch: 1 [2160/4399 (49%)] Training Loss: 2.854735\nCollecting data from Master Node: \n Train Epoch: 1 [2240/4399 (51%)] Training Loss: 2.808005\nCollecting data from Master Node: \n Train Epoch: 1 [2320/4399 (53%)] Training Loss: 2.836941\nCollecting data from Master Node: \n Train Epoch: 1 [2400/4399 (55%)] Training Loss: 2.745137\nCollecting data from Master Node: \n Train Epoch: 1 [2480/4399 (56%)] Training Loss: 2.856880\nCollecting data from Master Node: \n Train Epoch: 1 [2560/4399 (58%)] Training Loss: 2.731469\nCollecting data from Master Node: \n Train Epoch: 1 [2640/4399 (60%)] Training Loss: 2.721219\nCollecting data from Master Node: \n Train Epoch: 1 [2720/4399 (62%)] Training Loss: 2.849922\nCollecting data from Master Node: \n Train Epoch: 1 [2800/4399 (64%)] Training Loss: 2.779046\nCollecting data from Master Node: \n Train Epoch: 1 [2880/4399 (65%)] Training Loss: 2.758420\nCollecting data from Master Node: \n Train Epoch: 1 [2960/4399 (67%)] Training Loss: 2.882422\nCollecting data from Master Node: \n Train Epoch: 1 [3040/4399 (69%)] Training Loss: 2.731510\nCollecting data from Master Node: \n Train Epoch: 1 [3120/4399 (71%)] Training Loss: 2.764421\nCollecting data from Master Node: \n Train Epoch: 1 [3200/4399 (73%)] Training Loss: 2.784871\nCollecting data from Master Node: \n Train Epoch: 1 [3280/4399 (75%)] Training Loss: 2.701819\nCollecting data from Master Node: \n Train Epoch: 1 [3360/4399 (76%)] Training Loss: 2.717214\nCollecting data from Master Node: \n Train Epoch: 1 [3440/4399 (78%)] Training Loss: 2.809823\nCollecting data from Master Node: \n Train Epoch: 1 [3520/4399 (80%)] Training Loss: 2.745822\nCollecting data from Master Node: \n Train Epoch: 1 [3600/4399 (82%)] Training Loss: 2.725579\nCollecting data from Master Node: \n Train Epoch: 1 [3680/4399 (84%)] Training Loss: 2.845755\nCollecting data from Master Node: \n Train Epoch: 1 [3760/4399 (85%)] Training Loss: 2.785306\nCollecting data from Master Node: \n Train Epoch: 1 [3840/4399 (87%)] Training Loss: 2.979638\nCollecting data from Master Node: \n Train Epoch: 1 [3920/4399 (89%)] Training Loss: 2.891629\nCollecting data from Master Node: \n Train Epoch: 1 [4000/4399 (91%)] Training Loss: 2.852024\nCollecting data from Master Node: \n Train Epoch: 1 [4080/4399 (93%)] Training Loss: 2.661894\nCollecting data from Master Node: \n Train Epoch: 1 [4160/4399 (95%)] Training Loss: 2.668464\nCollecting data from Master Node: \n Train Epoch: 1 [4240/4399 (96%)] Training Loss: 2.776718\nCollecting data from Master Node: \n Train Epoch: 1 [4320/4399 (98%)] Training Loss: 2.782440\n\nTest set: Validation loss: 2.7715, Validation Accuracy: 20%\n\nSaving model: /kaggle/working/model_ProBert_4.pth \n\nCollecting data from Master Node: \n Train Epoch: 2 [0/4399 (0%)] Training Loss: 2.754643\nCollecting data from Master Node: \n Train Epoch: 2 [80/4399 (2%)] Training Loss: 2.831261\nCollecting data from Master Node: \n Train Epoch: 2 [160/4399 (4%)] Training Loss: 2.800264\nCollecting data from Master Node: \n Train Epoch: 2 [240/4399 (5%)] Training Loss: 2.714758\nCollecting data from Master Node: \n Train Epoch: 2 [320/4399 (7%)] Training Loss: 2.805909\nCollecting data from Master Node: \n Train Epoch: 2 [400/4399 (9%)] Training Loss: 2.767317\nCollecting data from Master Node: \n Train Epoch: 2 [480/4399 (11%)] Training Loss: 2.827338\nCollecting data from Master Node: \n Train Epoch: 2 [560/4399 (13%)] Training Loss: 2.829514\nCollecting data from Master Node: \n Train Epoch: 2 [640/4399 (15%)] Training Loss: 2.768362\nCollecting data from Master Node: \n Train Epoch: 2 [720/4399 (16%)] Training Loss: 2.765481\nCollecting data from Master Node: \n Train Epoch: 2 [800/4399 (18%)] Training Loss: 2.886671\nCollecting data from Master Node: \n Train Epoch: 2 [880/4399 (20%)] Training Loss: 2.870770\nCollecting data from Master Node: \n Train Epoch: 2 [960/4399 (22%)] Training Loss: 2.673584\nCollecting data from Master Node: \n Train Epoch: 2 [1040/4399 (24%)] Training Loss: 2.716464\nCollecting data from Master Node: \n Train Epoch: 2 [1120/4399 (25%)] Training Loss: 2.596757\nCollecting data from Master Node: \n Train Epoch: 2 [1200/4399 (27%)] Training Loss: 2.804610\nCollecting data from Master Node: \n Train Epoch: 2 [1280/4399 (29%)] Training Loss: 2.838282\nCollecting data from Master Node: \n Train Epoch: 2 [1360/4399 (31%)] Training Loss: 2.811563\nCollecting data from Master Node: \n Train Epoch: 2 [1440/4399 (33%)] Training Loss: 2.667871\nCollecting data from Master Node: \n Train Epoch: 2 [1520/4399 (35%)] Training Loss: 2.875289\nCollecting data from Master Node: \n Train Epoch: 2 [1600/4399 (36%)] Training Loss: 2.760957\nCollecting data from Master Node: \n Train Epoch: 2 [1680/4399 (38%)] Training Loss: 2.691081\nCollecting data from Master Node: \n Train Epoch: 2 [1760/4399 (40%)] Training Loss: 2.683902\nCollecting data from Master Node: \n Train Epoch: 2 [1840/4399 (42%)] Training Loss: 2.811726\nCollecting data from Master Node: \n Train Epoch: 2 [1920/4399 (44%)] Training Loss: 2.694576\nCollecting data from Master Node: \n Train Epoch: 2 [2000/4399 (45%)] Training Loss: 2.758231\nCollecting data from Master Node: \n Train Epoch: 2 [2080/4399 (47%)] Training Loss: 2.806021\nCollecting data from Master Node: \n Train Epoch: 2 [2160/4399 (49%)] Training Loss: 2.797767\nCollecting data from Master Node: \n Train Epoch: 2 [2240/4399 (51%)] Training Loss: 2.796900\nCollecting data from Master Node: \n Train Epoch: 2 [2320/4399 (53%)] Training Loss: 2.718183\nCollecting data from Master Node: \n Train Epoch: 2 [2400/4399 (55%)] Training Loss: 2.699186\nCollecting data from Master Node: \n Train Epoch: 2 [2480/4399 (56%)] Training Loss: 2.781447\nCollecting data from Master Node: \n Train Epoch: 2 [2560/4399 (58%)] Training Loss: 2.703327\nCollecting data from Master Node: \n Train Epoch: 2 [2640/4399 (60%)] Training Loss: 2.786418\nCollecting data from Master Node: \n Train Epoch: 2 [2720/4399 (62%)] Training Loss: 2.856276\nCollecting data from Master Node: \n Train Epoch: 2 [2800/4399 (64%)] Training Loss: 2.752870\nCollecting data from Master Node: \n Train Epoch: 2 [2880/4399 (65%)] Training Loss: 2.685897\nCollecting data from Master Node: \n Train Epoch: 2 [2960/4399 (67%)] Training Loss: 2.945720\nCollecting data from Master Node: \n Train Epoch: 2 [3040/4399 (69%)] Training Loss: 2.719743\nCollecting data from Master Node: \n Train Epoch: 2 [3120/4399 (71%)] Training Loss: 2.765503\nCollecting data from Master Node: \n Train Epoch: 2 [3200/4399 (73%)] Training Loss: 2.820817\nCollecting data from Master Node: \n Train Epoch: 2 [3280/4399 (75%)] Training Loss: 2.797672\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 309\u001b[0m\n\u001b[1;32m    307\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    308\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 309\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfrozen_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 214\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size, data_dir, test_batch_size, test_dir, frozen_layers, num_labels, lr, epsilon, weight_decay, epochs, log_interval, verbose, model_dir)\u001b[0m\n\u001b[1;32m    211\u001b[0m b_input_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    212\u001b[0m b_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 214\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, b_labels)\n\u001b[1;32m    217\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mProteinClassifier.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(output\u001b[38;5;241m.\u001b[39mpooler_output)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1014\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1015\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1016\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1020\u001b[0m )\n\u001b[0;32m-> 1021\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1034\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    418\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    425\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 426\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    436\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:348\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m         relative_position_scores_key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhrd,lrd->bhlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    346\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> 348\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}